---
title: "HW3"
author: "Jianyou Liu"
date: "October 13, 2018"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(hexbin)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 15,
  fig.height = 5,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_bw() + theme(legend.position = "bottom"))
```

## Problem 1

### Clean BRFSS dataset
```{r load_tidy_data}
data("brfss_smart2010")

# clean dataset
tidy_brfss = 
  janitor::clean_names(brfss_smart2010) %>% 
  filter(topic == "Overall Health", response == "Excellent"|response == "Very good"|response == "Good"|response == "Fair"|response == "Poor") %>% 
  rename(state = locationabbr, county = locationdesc, resp_id = respid)  %>% 
  mutate(response = as.factor(response)) %>% 
  arrange(match(response, c("Excellent", "Very good", "Good", "Fair", "Poor")))
  
```

### In 2002, which states were observed at 7 locations?
```{r p1_q1}
tidy_brfss %>% 
  filter(year == 2002) %>% 
  group_by(state) %>% 
  summarize(n = n_distinct(county)) %>% 
  filter(n == 7)
```
Based on the table above, CT, FL, and NC were observed 7 times.

### Spaghetti plot
```{r p1_q2}
# create "spaghetti plot"
tidy_brfss %>% 
  group_by(state, year) %>% 
  summarize(n = n_distinct(county)) %>% 
  ggplot(aes(x = year, y = n, color = state)) +
  geom_line() +
  labs(
    title = "Spaghetti Plot",
    x = "Year",
    y = "Number of Locations"
  ) +
   viridis::scale_color_viridis(
    name = "State", 
    discrete = TRUE
   )

```

### Table showing the **mean** and **standard deviation** of the proportion of "Excellent" responses across locations in NY State of years **2002**, **2006**, and **2010**.

```{r p1_q3}
# create table
tidy_brfss %>% 
  filter(state == "NY", year == 2002|year == 2006|year ==2010, response == "Excellent") %>%
  group_by(year, state) %>% 
  summarize(mean_excel_prop = mean(data_value), sd_excel_prop = sd(data_value)) %>% 
  knitr::kable(digits = 2)
```

### Computation and five-panel plot generation
```{r p1_q4}
# computation of average propotion in each response category
tidy2_brfss = select(tidy_brfss, year:county, response, data_value) %>% 
  spread(response, data_value) %>% 
  janitor::clean_names() %>% 
  group_by(year, state) %>% 
  summarize(avg_excel = mean(excellent), avg_very_good = mean(very_good), avg_good = mean(good), avg_fair = mean(fair), avg_poor = mean(poor)) %>% 
  gather(key = avg_response_type, value = resp_prop, avg_excel:avg_poor)

# make five-panel plot
ggplot(tidy2_brfss, aes(x = year, y = resp_prop, color = state)) +
  geom_line() +
  facet_grid(~avg_response_type) +
  labs (
    title = "Average Proportion for each Response Category",
    x = "Year", 
    y = "Response Proportion"
  )
  
```

# Problem 2

### Brief description and exploration of dataset
```{r load_data}
data("instacart")

dist_prod =  distinct(instacart, product_id, .keep_all = TRUE)
dist_user = distinct(instacart, user_id, .keep_all = TRUE)
```
This dataset comes from an online grocery store in NYC. The size of the dataset is `r dim(instacart)`, with `r nrow(instacart)` observations. There are `r nrow(dist_prod)` distinct products for sale and `r nrow(dist_user)` unique users. The order ids are the same for a single user. There are a total of 15 variables, of which *order id*, *product id*, and *user id* are key variables that identify observations. For example, the first row corresponds to the product "Bulgarian Yogurt" ordered from an user.


### How many aisles are there, and which aisles are the most items ordered from?
```{r p2_q1}
# number of unique aisles
nrow(distinct(instacart, aisle_id))

# aisles from which most items are ordered
instacart %>% 
  group_by(aisle_id) %>% 
  summarize(n_items = n()) %>% 
  arrange(desc(n_items))
```
There are `r nrow(distinct(instacart, aisle_id))` aisles and according to the table, aisle83 had the most items ordered from (150609).

### Plot of number of items ordered in each aisle
```{r p2_q2}
# create plot
instacart %>% 
  group_by(aisle_id) %>% 
  summarize(n_items = n()) %>%
  mutate(aisle_id = as.character(aisle_id)) %>% 
  ggplot(aes(x = as.character(aisle_id), y = n_items, color = aisle_id)) +
  geom_point(alpha = 0.5) +
  labs (
    title = "Number of Items vs. Aisle ID",
    x = "Aisle ID",
    y = "Number of Items"
  )
  viridis::scale_color_viridis(
    name = "Aisle ID", 
    discrete = TRUE
  )
```

### Most popular items
```{r p2_q3}
# table of most popular item ordered in specific aisle categories
instacart %>% 
  filter(aisle == "baking ingredients"| aisle == "dog food care"| aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>% 
  summarize(n = n()) %>% 
  filter(min_rank(desc(n)) < 2) %>% 
  knitr::kable()
```

Based on the table, the most popular items ordered from aisle "Baking Ingredients", "Dog food care", "packaged vegetable fruits" are "light brown sugar", "snack sticks chicken...", and "organic baby spinach" respectively.

### Mean hour of the day
```{r p2_q4}
# table of mean hour of day at which some products are ordered on each day of the week
instacart %>% 
  filter(product_name == "Pink Lady Apples"| product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  spread(key = order_dow, value = mean_hour) %>% 
  rename(Sunday = '0', Monday = '1', Tuesday = '2', Wednesday = '3', Thursday = '4', Friday = '5', Saturday = '6')%>% 
  knitr::kable(digits = 2)

```

# Problem 3
### Short description
```{r load_p3_data}
data("ny_noaa")
```
The size of this dataset is `r dim(ny_noaa)`. There are a total of `r nrow(ny_noaa)` observations with each row corresponding to a observation from a weather station on a single day. There are `r nrow(distinct(ny_noaa, id))` unique weather stations. The total number of variables is `r ncol(ny_noaa)`, of which "prcp", "snow", "snwd", "tmax", and "tmin" are key variables. The extent to which missing values is an issue is very large because there are `r sum(is.na(ny_noaa))` missing values in the entire dataset.

### Data cleaning
```{r tidy_p3_data}
tidy_nynoaa = 
  separate(ny_noaa, date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(tmin = as.numeric(tmin), tmax = as.numeric(tmax), prcp = as.numeric(prcp), snow = as.numeric(snow), snwd = as.numeric(snwd)) %>% 
  mutate(prcp = prcp/10, tmax = tmax/10, tmin = tmin/10)
```
I cleaned the dataset by separating *year*, *month*, and *day* into distinct variables, converted quantitative variables into numeric, and recomputed observations for *prcp*, *tmin*, and *tmax* to reasonable units mm and degrees celsius as opposed to the initial 1/10th of mm and 1/10th of degrees celsius.

### Most common value for snowfall
```{r p3_q1}
tidy_nynoaa %>% 
  group_by(snow) %>% 
  summarize(n = n()) %>% 
  arrange(desc(n))
```
Based on the table, the most commonly observed value is 0, because throughout the days in a year, there is no snowfall in the majority of times.

### Two-panel plot for average tmax
```{r p3_q2}
# create 2-panel plot
tidy_nynoaa %>% 
  filter(month == "01"| month == "07") %>% 
  group_by(id, year, month) %>% 
  summarize(avg_tmax = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = avg_tmax)) +
  geom_point(alpha = .5) +
  facet_grid(~month) +
  labs (
    title = "Average tmax in January and July across Years",
    x = "Year",
    y = "Average tmax"
  )
  
```

According to the plot, the average max temperatures for each station across years are generally lower in January than in July. In January, the mean max temperatures are scattered around 0 degrees while those for July are scattered around 25 degrees. There seems to be an outlier value in January of 1982 of around -15 degrees, and another one in July of 1988 of around 15 degrees.

### Juxtaposition of different plots using patchwork
```{r p3_q3}
# create plot for tmax vs. tmin
p1 = tidy_nynoaa %>% 
  ggplot(aes(x = tmax, y = tmin)) +
  geom_hex() +
  labs(
    title = "tmax vs. tmin"
  )

# create plot for distribution of snowfall between 0 and 100mm
p2 = tidy_nynoaa %>% 
  filter(snow < 100 & snow > 0) %>% 
  ggplot(aes(x = snow, fill = year)) +
  geom_density(alpha = .5) +
  labs(
    title = "Snowfall Distribution by Year ",
    x = "Snowfall(mm)"
  )
  
# use patchwork package to combine panels
p1 + p2
```

